# 常见loss函数总结

因为在pytorch中`torch.log()` 是以e为底的对数函数，所以关于熵的一些损失函数的反向传播求导时处理就以$ln(x)$作为$log(x)$进行处理

## BCE Loss

二分类交叉熵，用于二分类任务中，公式如下，其中$y_i$表示样本$i$的标签，正类为1，负类为0，*$p_i$*表示样本$i$预测为正的概率。
$$
Loss = -\frac{1}{N}\sum_i[y_i \cdot log(p_i)+(1-y_i)\cdot log(1-p_i)]
$$
因为*$p_i$*表示样本$i$预测为正的概率，所以很明显$p_i$的取值范围为[0,1]，所以BCE loss常与sigmoid激活函数配合使用，比如pytorch中的`BCEWithLogiticsLoss `就是将两者合在一起。BCE loss 一般也可用于**多标签分类**中，也就是对于分类结果如果**类别间不互斥**，则就只能采用 "sigmoid + BCE"，（此时BCE Loss的输出$p_i$  就表示为当前样本属于类别$i$的概率，所以要与二分类时的情况加以区分），而对于分类结果是**类别间互斥**的情况(只能有一类胜出，即标签是**one-hot**)，“sigmoid+BCE”化为多个二分类问题与“softmax+CE”直接进行分类都是可以被用到的方法。

> 这里需要区分以下：
>
> * 对于Sigmoid + BCE用于多标签分类的情况，一组$X= ({x_0,x_1,...,x_c})$仅表示某个样本的分类情况，总共有$c$类，对应的真值Label  $Y=(y_0,y_1,...,y_c)$则表示某个样本的真实多分类target 
>
> * 对于Sigmoid + BCE用于多标签分类的情况，一组$X= ({x_0,x_1,...,x_n})$则表示n个数据的属于正类的概率，对应的真值Label$Y=(y_0,y_1,...,y_n)$ 表示多个样本的真实二分类target 

* **反向传播梯度**

假设有一组数据$X= ({x_0,x_1,...,x_n})$,经过sigmoid激活函数处理后，得到预测结果输出为$X'=(x'_0,x'_1,...,x'_n)$ ，其中$x'_i=σ(x_i)$  ,$σ(x)=sigmoid$   ，之后与数据的真值label   $Y=(y_0,y_1,...,y_n)$参与计算二分类的loss，代入BCE Loss，得到如下表达式
$$
Loss =-\frac{1}{N}\sum_i[y_i \cdot log(x'_i)+(1-y_i)\cdot log(1-x'_i)]
$$
反向求导可知
$$
\begin{aligned}
\frac{\partial L}{\partial x_i}=& \quad \frac{\partial L}{\partial x'_i}\cdot\frac{\partial x'_i}{\partial x_i} \\
 =& -\frac{1}{N}[\frac{y_i}{x'_i}-(1-y_i)\cdot \frac{1}{1-x'_i}] \cdot(x'_i(1-x'_i)) \\
 =&\quad \frac{1}{N}[x'_i(1-y_i) - (1-x'_i)y_i] \\
 =&\quad \frac{1}{N}[x'_i - y_i]
 \end{aligned}
$$
式中$x'=sigmoid(x)$，且${\partial x'_i}/{\partial x_i}=(x'_i(1-x'_i))$，也就是sigmoid函数的求导详见**激活函数总结**。所以通过sigmoid+BCE Loss回传的梯度是**正比于**预测值$x'_i$与真值$y_i$之差的，所以在预测值与真实值**差异较大时参数调整就越快**，收敛就越快，同时，梯度也不在与sigmoid的梯度有关了，我们都知道因为sigmoid函数的性质所以导致在反向传播时容易出现**梯度消失**的情况，所以使用BCE Loss在一定程度上防止了梯度消失的问题

* **Pytorch代码示例**

```python
# pytorch对应类为 torch.nn.BCELoss(weight=None,size_average=None, reduce=None, reduction: str = 'mean')
# weight 表示每个类别的loss的权重，shape必须与target一致
# reduce为False，size_average不起作用，返回向量形式的loss。
# 如果reduce为True，size_average为True，返回loss的均值，即loss.mean()。
# 如果reduce为True，size_average为False，返回loss的和，即loss.sum()。
# 如果reduction = ‘none’，直接返回向量形式的 loss。
# 如果reduction = ‘sum’，返回loss之和。
# 如果reduction = ''mean，返回loss的平均值

import torch
import torch.nn as nn


m = nn.Sigmoid()
loss = nn.BCELoss()#loss 以均值形式计算
input = torch.randn(3, requires_grad=True)
input_a = m(input) # 如 tensor([0.2994, 0.6109, 0.8609], grad_fn=<SigmoidBackward0>)
target = torch.Tensor([1,0,0])
output = loss(input_a, target) #tensor(1.3742, grad_fn=<BinaryCrossEntropyBackward0>)
output.backward()

print(input.grad) 
# tensor([-0.2335,  0.2036,  0.2870])
# 简单计算不难发现梯度满足 [1/3*(0.2994-1),1/3*(0.6109-0),1/3*(0.8609-0)]
# 进而证明关于梯度的推导是正确的
```



## CE Loss

交叉熵损失函数，在多分类的情况中经常使用，这里的多分类就是指类别间互斥的情况所以其参与计算的label数据只能有一类“胜出”，所以CE Loss一般与softmax一起用，比如pytorch中的`CrossEntropyLoss`就是softmax 和交叉熵的结合。交叉熵公式如下，其中$c$表示最终分类的类别数，$x_i$表示样本预测为类别$i$的概率，$y$表示为样本真值。
$$
\begin{aligned}
Loss =&-\sum_{i=1}^Cy_i\cdot log(softmax(x_i))\\\\
     =&-\sum_{i=1}^Cy_i\cdot log(\frac{exp(x_i)}{\sum_{i=1}^{C}exp(x_i)})
\end{aligned}
$$

> 注意：**以下公式没有考虑多Batch的情况，对多Batch的loss则是将单个以下单个Batch公式得到loss求平均即可**
>
> 而在求反向传播梯度时也就引入了求平均的梯度1/N

* **反向传播梯度**

假设某个数据的预测结果$Z= ({z_0,z_1,...,z_c})$，经过softmax函数之后得到归一化之后的预测结果$z'=(z'_0,z'_1,...,z'_c)$，之后与数据的真值label  $Y=(y_0,y_1,...,y_c)$带入CE Loss求交叉熵损失

$$
\begin{aligned}
Loss =& -\sum_{i=1}^Cy_i\cdot log(z'_i)\\\\
=&-\sum_{i=1}^Cy_i\cdot log(\frac{exp(z_i)}{\sum_{i=1}^{C}exp(z_i)})
 \end{aligned}
$$

在看反向求导可得，可以发现，CE Loss 函数的求导本身比较容易得到，关键点在于softmax的求导(${\partial z'_i}/{\partial z_i}$)，
$$
\begin{aligned}
\frac{\partial L}{\partial z_i}=& \quad \frac{\partial L}{\partial z'_i}\cdot\frac{\partial z'_i}{\partial z_i} \\
 =& -y_i \frac{1}{z'_i} \cdot \frac{\partial z'_i}{\partial z_i}   \qquad  \qquad \qquad (1)
 \end{aligned}
$$

* **Softmax 的反向传播**

https://blog.csdn.net/bqw18744018044/article/details/83120425



* **Pytorch代码实现**

```python
# Example of target with class indices
loss = nn.CrossEntropyLoss()
input = torch.randn(3, 5, requires_grad=True)
target = torch.empty(3, dtype=torch.long).random_(5)
output = loss(input, target)
output.backward()

# Example of target with class probabilities
input = torch.randn(3, 5, requires_grad=True)  
target = torch.randn(3, 5).softmax(dim=1)
output = loss(input, target)
output.backward()
```



## KL散度

以离散随机变量为例，若有一个离散随机变量$X$的可能取值为$X=(x_1,x_2,...,x_n)$，而对应的概率为$p_i=p(X=x_i)$，则随机变量$X$的**熵定义**为：
$$
H(X) = -\Sigma p(x)log (p(x))
$$
若有两个概率分布分别为$p(x)$、$q(x)$，其中$p(x)$为真实分布，$q(x)$为预测分布，则使用$q(x)$来近似$p(x)$的交叉熵定义为
$$
H(p,q) =-\Sigma p(x)log(q(x))
$$
而KL散度又称之为相对熵，定义为
$$
D_{KL}(p||q) = H(p,q)-H(p)
             = \Sigma p(x) log(\frac{p(x)}{q(x)})
$$

* **反向梯度传播**

假设有一组数据的预测结果为$Z= ({z_0,z_1,...,z_n})$ , 数据的真值label $Y=(y_0,y_1,...,y_n)$ ,带入求KL散度
$$
L_{KL} = \sum_i^n y_i  \cdot log(\frac{y_i}{z_i})
$$
反向求导
$$
\begin{aligned}
\frac{\partial L}{\partial z_i} =& y_i \cdot (\frac{z_i}{y_i}) \cdot(-\frac{y_i}{z_i^2}) \\
=& -\frac{y_i}{z_i}

 \end{aligned}
$$
KL散度与交叉熵梯度是一样的？



## MSE



## L1 & L2 &Smooth L1



## Focal Loss

