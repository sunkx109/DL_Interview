# 常见激活函数总结

## Sigmoid

函数公式如下
$$
sigmoid(x)= \frac{1}{1+e^{-x}}
$$
函数图像如下

![sigmoid](%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB.assets/sigmoid.png)

反向求导
$$
\begin{aligned}
sigmoid'(x)=& (\frac{e^x}{e^x+1})'  \\\\
=&(\frac{e^x(e^x+1)-e^{2x}}{(e^x+1)^2})\\
=&(\frac{1}{e^x+2+e^{-x}})\\\\
=&sigmoid(x)(1-sigmoid(x))



\end{aligned}
$$
梯度图像

![avatar](%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB.assets/grad_sigmoid.png)

Sigmoid函数的三个主要缺陷：

* 梯度消失：注意：**Sigmoid 函数趋近 0 和 1 的时候变化率会变得平坦**，也就是说，Sigmoid 的梯度趋近于 0。神经网络使用 Sigmoid 激活函数进行反向传播时，输出接近 0 或 1 的神经元其梯度趋近于 0。这些神经元叫作饱和神经元。因此，这些神经元的权重不会更新。此外，与此类神经元相连的神经元的权重也更新得很慢。该问题叫作梯度消失。因此，想象一下，如果一个大型神经网络包含 Sigmoid 神经元，而其中很多个都处于饱和状态，那么该网络无法执行反向传播。

* 不以零为中心：Sigmoid 输出不以零为中心的。

* 计算成本高昂：exp() 函数与其他非线性激活函数相比，计算成本高昂。

## Tanh

公式
$$
tanh(x)=\frac{1-e^{-2x}}{1+e^{-2x}}
$$
函数图像

![avatar](%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB.assets/tanh.png)

梯度图像

![avatar](%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB.assets/grad_tanh.png)

## ReLU

![avatar](%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB.assets/relu.png)

ReLU的一些缺陷：

* 不以零为中心：和 Sigmoid 激活函数类似，ReLU 函数的输出不以零为中心。

*  前向传导过程中，如果 x < 0，则神经元保持非激活状态，且在反向传播中「杀死」梯度。这样权重无法得到更新，网络无法学习。当 x = 0 时，该点的梯度未定义，但是这个问题在实现中得到了解决，通过采用左侧或右侧的梯度的方式。

为了解决 ReLU 激活函数中的梯度消失问题，当 x < 0 时，我们使用 Leaky ReLU——该函数试图修复 dead ReLU 问题。

## Leaky ReLU

公式
$$
f(x) =max(0.1x,x)
$$
图像

![avatar](%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB.assets/LeakyRelu.png)

当 x < 0 时，它得到 0.1 的正梯度。该函数一定程度上缓解了 dead ReLU 问题，但是使用该函数的结果并不连贯。尽管它具备 ReLU 激活函数的所有特征，如计算高效、快速收敛、在正区域内不会饱和。

当然也可以对LeakyRelu做一些拓展，如在副区间的时候让x乘以一个可学习的参数α

## Swish

$$
Swish(x) = \frac{x}{1+e^{-x}}
$$

![avatar](%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB.assets/swish.png)

## Mish

$$
f(x)=x*tanh(ln(1+x))
$$

![avatar](%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB.assets/Mish.png)

s